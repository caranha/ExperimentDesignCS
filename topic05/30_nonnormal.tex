\section{Non-normal Data}

\begin{frame}

  \begin{center}
    {\bf Part II -- Non-normal Data}
  \end{center}

\end{frame}

\begin{frame}{Non Normality}{What is non-normality?}
  \begin{itemize}
    \item Until now we studied methods which {\bf assume} that the experimental
      data follows a normal distribution (or close enough).\bigskip
    \item In many cases, this assumption {\bf does not hold}. In this condition,
      how can we perform the statistical analysis of the results?
  \end{itemize}
\end{frame}

\subsection{Motivation Example}

\begin{frame}[fragile]{Non Normal data makes everything go wrong}{Weight Loss Example}
A researcher is examining two different diets, {\bf Diet A} and {\bf Diet B},
and wants to compare the weight loss by people following one diet or the other.
They obtained the following data:

\begin{verbatim}
diet.a <- c(4,3,0,-3,-4,-5,-11,-14,-15,-300)
diet.b <- c(-8,-10,-12,-16,-18,-20,-21,-24,-26,-30)
\end{verbatim}
\bigskip

As you can see, Diet A has one big outlier\footnote{Why does this outlier exist? Data input error? Very rare case?} that makes the data not normal. How much does this affect the statistical test?
\end{frame}

\begin{frame}{Non Normal data makes everything go wrong}{Weight Loss Example}
  \begin{columns}
    \column{.5\textwidth}
    Data with Outlier\\
    \includegraphics[width=.8\textwidth]{../img/diet_outlier1}
    \column{.5\textwidth}
    Data without Outlier\\
    \includegraphics[width=.8\textwidth]{../img/diet_outlier2}
  \end{columns}\bigskip

  Checking a visualization, it seems like diet A has smaller losses than diet B
  overall. Except for that outlier. What happens with the T-test?
\end{frame}

\begin{frame}[fragile]{Non Normal data makes everything go wrong}{Weight Loss Example}
The standard T-test does not indicate a difference between these samples,
and even suggests that the mean of the first sample is lower!\bigskip

{\smaller
\begin{verbatim}
diet.a <- c(4,3,0,-3,-4,-5,-11,-14,-15,-300)
diet.b <- c(-8,-10,-12,-16,-18,-20,-21,-24,-26,-30)
t.test(diet.a,diet.b)

##  Welch Two Sample t-test
##
## data:  diet.a and diet.b
## t = -0.53945, df = 9.1048, p-value = 0.6025
## alternative hypothesis: true difference in
##                         means is not equal to 0
## 95 percent confidence interval:
##  -82.9774  50.9774
## sample estimates:
## mean of x mean of y
##     -34.5     -18.5
\end{verbatim}}
\end{frame}



\begin{frame}{Non Normal data makes everything go wrong}{How can we solve this problem?}

  \begin{itemize}
    \item Identify and remove the outlier?
    \begin{itemize}
      \item If the outlier is an experimental error, it makes sense to remove it;
      \item On the other hand, maybe the outlier is some important effect worth being investigated;
    \end{itemize}
    \bigskip

    \item Alternatively, we could use a test that is not sensitive to the outlier.
  \end{itemize}

\end{frame}


\begin{frame}[fragile]{Non Normal data makes everything go wrong}{Weight Loss Example}

A {\bf non-parametric test} will indicate a rejection of the null hypothesis of equality.

{\small
\begin{verbatim}
diet.a <- c(4,3,0,-3,-4,-5,-11,-14,-15,-300)
diet.b <- c(-8,-10,-12,-16,-18,-20,-21,-24,-26,-30)
wilcox.test(diet.a,diet.b)

##  Wilcoxon rank sum test
##
## data:  diet.a and diet.b
## W = 82, p-value = 0.01469
## alternative hypothesis: true location shift
## is not equal to 0
\end{verbatim}}
\end{frame}

\subsection{Concepts and Examples}

\begin{frame}{Examples of Non-Normal Data}
  There are many different ways that data can violate the assumption of normality:\bigskip

  \begin{itemize}
    \item \emph{Special Observations in the Data}:
      \begin{itemize}
        \item Outliers, data collection errors;
        \item Absolute limits in the data (measuring time);
      \end{itemize}

    \item \emph{Extreme Non-Normal Distributions:}
    \begin{itemize}
      \item Power Distribution, Cauchy Distribution, etc.
    \end{itemize}

    \item \emph{Ordinal Data:}
    \begin{itemize}
      \item Ordinal data is data that can be ordered and compared by some criteria, but you cannot apply traditional algebra on it (ex: subjective scores);
    \end{itemize}

    \item \emph{Completely Non-numerical data}:
    \begin{itemize}
      \item categorical data, class data, etc; (ex: colors)
    \end{itemize}
  \end{itemize}
\end{frame}


\begin{frame}{Non-Normal Data Example: Random Processes}

  Random processes in nature, such as plant growth or shell formation, are often seen to follow a normal distribution or bell curve. On the other hand, random artificial processes not always do.\bigskip

  \begin{itemize}
    \item In general, Pseudo Random Number Generators will use an {\bf Uniform Distribution}. Because of the CLT, aggregations of these results will tend to normal distributions.\medskip

    \item On the other hand, random social processes will often show {\bf Power Distributions} (salaries, social networks) or {\bf Binomial Distributions} (queues);\medskip

    \item It is important to study and understand the process being researched to know its characteristics;
  \end{itemize}
\end{frame}

\begin{frame}{Non Normality}{Example: Likert Data}
% TODO: Add a slide or section about how we should deal with
% LIKERT data.

\emph{Likert} data is often collected from surveys and interview questions. It
is usually  composed of multiple questions with 5 or 7 options, ranged from
"Strongly Agree" to "Strongly Disagree", or "Always" to "Never".\bigskip

\begin{center}
  \includegraphics[width=0.9\textwidth]{../img/likert_scale}
\end{center}

Why can't we treat likert data directly as numerical?
\begin{itemize}
  \item Values outside of the 0-5 range have no meaning;
  \item Algebra on likert data has no meaning (Neutral+Disagree=?)
  \item The difference between levels is not clear. Is "Agree" equally
  distant from "Slightly Agree" and "Neutral"?
\end{itemize}
\end{frame}

% \begin{ftstf}{Non Normality}{Example: Fully Symbolic Data}
% TODO: Expand on this example
%   Some data may not have a clear numerical correspond
% \end{ftstf}


\begin{frame}{Non Normality}{Strategies for non normal data}
  When our data does not follow the normality assumption, there are many different strategies that we can apply, depending on the type of data, and the type of normality violation:\bigskip

  \begin{itemize}
    \item {\bf Do Nothing}
    \begin{itemize}
      \item We can remove outliers that break the normality assumption, or trust that test will be robust for small deviations of normality;
    \end{itemize}
    \item {\bf Transform the Data}
      \begin{itemize}
        \item Transformation of the data can restore the normal property to data;
      \end{itemize}
    \item {\bf Non parametric Testing}
    \begin{itemize}
      \item Some statistical tests do not make the assumption of normality of the sampling distribution (in exchange for less power);
    \end{itemize}
    \item {\bf Look for a new statistics textbook}
  \end{itemize}\bigskip

\end{frame}

% TODO: DO NOTHING SECTION
%    - remove outliers
%    - Trusting the CLT
%    - You need to understand WHY your data is not normal ()
% TODO: How to report on non-normal data
% TODO: FIXME: separate the lecture in *skewed* data and *ordinal* data.
% \begin{ftstf}{Non Normality}{Reporting on non-normal data} %% What you need to write on the paper
%   \begin{itemize}
%     \item Mean, Median, Ranks
%     \item Standard Deviation and IQR
%   \end{itemize}
% \end{ftstf}
%
% TODO: When is it ok to ignore small violations of normality?
% \section{Do nothing}
%
% \begin{ftstf}{Strategy 1 - Do nothing}{Ignoring the assumption of normality.}
%   \begin{itemize}
%     \item Very high number of samples is available (how high?);
%     \item The other conditions must still hold;
%   \end{itemize}
% \end{ftstf}
% TODO: R-code: some normal data with high n do not pass a normality test.
% findNonNormal <- function(n = 5000){
%     p <- 1
%     while(p > 0.05) {
%         y <- rnorm(n)
%         p <- shapiro.test(y)$p.value
%         }
%     y
%     }
%
% y <- findNonNormal()
% hist(y)
% qqnorm(y)


\section{Data Transformation}

\begin{frame}{Data Transformation}{Log Transformation}
  We can apply certain transformations to "normalize" some data.
  \begin{center}
    \includegraphics[width=0.75\textwidth]{../img/lognormal_transformation}
  \end{center}
\end{frame}

%% TODO: Add images here
\begin{frame}[fragile]{Log Transformation}{}
\begin{verbatim}
# Generate lognormal data
#
set.seed(17)
z <- exp(rnorm(200, -2, 0.4))

# Log transformation
#
y <- log(z)
mu.hat <- mean(y)
sigma.hat <- sd(y)
\end{verbatim}
\end{frame}

\begin{frame}{Data Transformation}{Skew Transformation}
  A strong skew in the sampling distribution can be a larger problem for the standard statistical tests. It is possible to remove these through data transformations:\bigskip

  \begin{itemize}
    \item For left skewed data:
    \begin{itemize}
      \item square root, cube root, log
    \end{itemize}
    \item For right skewed data:
    \begin{itemize}
      \item square root (constant $-x$), cube root (constant $-x$)
    \end{itemize}
  \end{itemize}\bigskip

  {\bf Attention:} Logarithm of 0 and negative data is not defined, so you may need to add a constant before the transformation.
\end{frame}
% TODO: Example of transformation to remove skew



\begin{frame}{Data Transformation}{Be careful when transforming data}
  \begin{itemize}
    \item Pay attention when describing the analysis on a paper or report:
    \begin{itemize}
      \item Any transformation used must be explained in the analysis;
      \item The discussion of the results must be done on the transformed data as well as the original data;
    \end{itemize}\bigskip

    \item Beware that the hypotheses may not be equivalent!
    \begin{itemize}
      \item Example: The lognormal mean includes the variance. But the transformed lognormal mean does not. In this case, the null hypothesis is only equivalent when the variance of the transformed distribution is equal!
    \end{itemize}
  \end{itemize}
\end{frame}
%% TODO: Example of the mistakes above -- what wrong conclusions you can reach if you don't do this.

\subsection{Bootstrapping}

\begin{frame}{Bootstrapping}{}

  The Bootstrapping procedure is used to obtain an approximation of the "sample mean distribution" from the sample data.\bigskip

  Following the properties of the Central Limit Theorem, the sample mean distribution will usually follow a normal distribution, even when the underlying distribution of observation values is not normal;
\end{frame}

\begin{frame}{The Bootstrapping Procedure}
  \begin{itemize}
    \item Take an initial sample with $m$ observations;
    \item Create $n$ {\bf bootstrap samples} by selecting $m_b < m$ from the initial sample $n$ times;
    \item Calculate the mean of each bootstrap sample.
    \item The set of bootstrap sample means are your {\bf bootstrapped data};

    \begin{center}
      \includegraphics[width=.8\textwidth]{../img/bootstrap_lowres}
    \end{center}
  \end{itemize}
\end{frame}

% TODO: Code Example


\begin{frame}[fragile]{Generating Bootstrapped Data}
  The R package "boot" creates bootstrapped data, c.i.s, and tests.
  \begin{columns}[T]
    \column{0.7\textwidth}
    {\smaller
\begin{verbatim}
> city
     u   x
1  138 143
2   93 104
3   61  69
4  179 260
5   48  75
6   37  63
7   29  50
8   23  48
9   30 111
10   2  50

> library(boot)
> ratio <- function(d, w) sum(d$x * w) / sum (d$u * w)
> bootstrap <- boot(city, ratio, R = 999, stype = "w")
> city.bootstrap <- bootstrap[[2]]
\end{verbatim}
    }
    \column{0.3\textwidth}
    \includegraphics[width=1\textwidth]{../img/bootstrap_original}
    \includegraphics[width=1\textwidth]{../img/bootstrap_histogram}
  \end{columns}
\end{frame}

\section{Non-Parametric Tests}

\begin{frame}{Non Parametric Tests}{}
  Non-parametric Tests involve statistics that do not assume normality from the population distribution.\bigskip

  Weak assumptions about the population, however, causes the non-parametric tests to be less strong than parametric ones. Also, usually non-parametric statistics usually do not calculate the distance between the parameter estimate and the hypothesis values.
  \bigskip

  \begin{itemize}
    \item Wilcoxon Signed Rank Test (1 sample)
    \item Wilcoxon Ranked Sum Test / Mann-whitney Test (2 samples)
    \item Kruskall-Wallis Test (multiple samples)
  \end{itemize}
\end{frame}

% \begin{frame}{Non Parametric Tests}{Basic Idea}
%   % TODO: Generally explain the basic idea of non-parametric tests
% \end{frame}

\subsection{1 or 2 samples}

% \begin{frame}{Non Parametric Tests}{One or two Samples}
%   \begin{itemize}
%     \item unpaired test: Mann-Whitney U-test;
%     \item paired test: Wilcoxon signed-ranks test;
%   \end{itemize}
% \end{frame}

\begin{frame}{Mann-Whitney U-test}{Unpaired test for two samples}
  \begin{center}
    \includegraphics[width=1\textwidth]{../img/MannWhitneyU}
  \end{center}
\end{frame}

\begin{frame}{Mann-Whitney U-test}{}
  \begin{center}
    \includegraphics[width=.6\textwidth]{../img/MannWhitneyU}
  \end{center}

  \begin{itemize}
    \item Choose the smaller value of U or U'
    \item Null Hypothesis: {\bf Both samples come from the same distribution}
    \item Under the null hypothesis, for big enough $n_1$ and $n_2$, U follows
    roughly a normal distribution with mean $\frac{n_1n_2}{2}$ and variance $\frac{n_1n_2(n_1+n_2+1)}{12}$
    \item Calculate the test statistic z, and find the p-value from the $\alpha$-percentile in the z distribution.
  \end{itemize}
\end{frame}

\begin{frame}{Wilcoxon Signed Rank Test}{}
  \includegraphics[width=.4\textwidth]{../img/SignTest}

  \begin{itemize}
    \item The Wilcoxon test takes the relative difference between pairs (positive or negative)
    \item Null hypothesis: {\bf Positive and Negative signs are equally likely}
    \item The overall number of signs is compared against a binomial distribution under the Null hypothesis.
  \end{itemize}
\end{frame}


\begin{frame}[fragile]{Wilcoxon Signed Rank Test}{R example}

{\smaller
\begin{verbatim}
## Hollander & Wolfe (1973), 29f.
## Hamilton depression scale factor measurements in 9
## patients with mixed anxiety and depression, taken at
## the first (x) and second (y) visit after initiation
## of a therapy (administration of a tranquilizer).

x <- c(1.83,  0.50,  1.62,  2.48, 1.68, 1.88,
       1.55, 3.06, 1.30)
y <- c(0.878, 0.647, 0.598, 2.05, 1.06, 1.29,
       1.06, 3.14, 1.29)

wilcox.test(x, y, paired = TRUE, alternative = "greater")

Wilcoxon signed rank test
data:  x and y
V = 40, p-value = 0.01953
alternative hypothesis: true location shift
is greater than 0
\end{verbatim}}
\end{frame}

%% TODO: Move this to the multiple samples (ANOVA) lecture
% \subsection{Multiple Samples}
%
% \begin{ftstf}{Non Parametric Tests}{Multiple Samples}
%   \begin{itemize}
%     \item unpaired test: Kurskal-Wallis test
%     \item paired test: Friedman test
%   \end{itemize}
% \end{ftstf}
